{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import librosa \n",
    "import sys\n",
    "import glob \n",
    "import random\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "from tacotron.utils import get_spectrograms\n",
    "import pandas as pd\n",
    "import gc \n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def read_speaker_info(speaker_info_path):\n",
    "    speaker_ids = []\n",
    "    with open(speaker_info_path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            speaker_id = line.strip().split()[0]\n",
    "            speaker_ids.append(speaker_id)\n",
    "    return speaker_ids\n",
    "\n",
    "\n",
    "def wave_feature_extraction(wav_file, sr):\n",
    "    y, sr = librosa.load(wav_file, sr)\n",
    "    y, _ = librosa.effects.trim(y, top_db=20)\n",
    "    return y\n",
    "\n",
    "def spec_feature_extraction(wav_file):\n",
    "    mel, mag = get_spectrograms(wav_file)\n",
    "    return mel, mag \n",
    "\n",
    "\n",
    "def sample_single_segments(pickle_path,sample_path,segment_size,n_samples):\n",
    "\n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    # (utt_id, timestep, neg_utt_id, neg_timestep)\n",
    "    samples = []\n",
    "\n",
    "    # filter length > segment_size\n",
    "    utt_list = [key for key in data]\n",
    "    utt_list = sorted(list(filter(lambda u : len(data[u]) > segment_size, utt_list)))\n",
    "    print(f'{len(utt_list)} utterances')\n",
    "    sample_utt_index_list = random.choices(range(len(utt_list)), k=n_samples)\n",
    "\n",
    "    for i, utt_ind in enumerate(sample_utt_index_list):\n",
    "        if i % 500 == 0:\n",
    "            print(f'sample {i} samples')\n",
    "        utt_id = utt_list[utt_ind]\n",
    "        t = random.randint(0, len(data[utt_id]) - segment_size)\n",
    "        samples.append((utt_id, t))\n",
    "\n",
    "    with open(sample_path, 'w') as f:\n",
    "        json.dump(samples, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ids:  ['awb', 'bdl', 'clb', 'jmk', 'ksp', 'ABA', 'SKA', 'BWC', 'LXC', 'ASI', 'SVBI', 'HKK', 'HJK', 'EBVS', 'MBMPS', 'HQTV', 'PNV']\n",
      "Testing ids:  ['slt', 'YBAA', 'NCC', 'RRBI', 'YDCK', 'ERMS', 'THV']\n"
     ]
    }
   ],
   "source": [
    "l1arctic_training_ids=[a[44:47] for a in sorted(glob.glob(\"/raid/users/ayesilkanat/MSC/L1Arctic/*_arctic\"))]\n",
    "l2arctic_training_ids=['ABA','SKA','BWC','LXC','ASI','SVBI','HKK','HJK','EBVS','MBMPS','HQTV','PNV']\n",
    "l1arctic_test_speaker_ids=[l1arctic_training_ids.pop(),l1arctic_training_ids.pop()]\n",
    "l1arctic_test_speaker_ids.pop() # leave one for validation one for test\n",
    "l2arctic_test_speaker_ids=['YBAA', 'NCC', 'RRBI', 'YDCK', 'ERMS', 'THV']\n",
    "test_speaker_ids = l1arctic_test_speaker_ids +l2arctic_test_speaker_ids \n",
    "train_speaker_ids = l1arctic_training_ids + l2arctic_training_ids\n",
    "\n",
    "print(\"Training ids: \",train_speaker_ids)\n",
    "print(\"Testing ids: \",test_speaker_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['awb', 'bdl', 'clb', 'jmk', 'ksp', 'ABA', 'SKA', 'BWC', 'LXC', 'ASI', 'SVBI', 'HKK', 'HJK', 'EBVS', 'MBMPS', 'HQTV', 'PNV', 'slt', 'YBAA', 'NCC', 'RRBI', 'YDCK', 'ERMS', 'THV'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stage=0\n",
    "segment_size=128\n",
    "n_out_speakers=20\n",
    "test_prop=0.1\n",
    "sample_rate=24000\n",
    "training_samples=10000000\n",
    "testing_samples=10000\n",
    "n_utt_attr=5000\n",
    "\n",
    "output_dir = \"/raid/users/ayesilkanat/MSC/adaptive-accent-conversion/setup3/spectrograms/sr_24000_mel_norm_128frame_256mel\"\n",
    "test_proportion = test_prop\n",
    "\n",
    "n_utts_attr = n_utt_attr\n",
    "\n",
    "\n",
    "speaker2filenames = defaultdict(lambda : [])\n",
    "\n",
    "\n",
    "for tr_id in l1arctic_training_ids:\n",
    "    for path in sorted(glob.glob(\"/raid/users/ayesilkanat/MSC/L1Arctic/cmu_us_\"+tr_id+\"_arctic/wav/*.wav\")):\n",
    "        speaker2filenames[tr_id].append(path)\n",
    "\n",
    "\n",
    "for tst_id in l2arctic_training_ids:\n",
    "        \n",
    "    for path in sorted(glob.glob(\"/raid/users/ayesilkanat/MSC/ArcticL2/\"+tst_id+\"/wav/*.wav\")):\n",
    "        speaker2filenames[tst_id].append(path)\n",
    "        \n",
    "for tr_id in l1arctic_test_speaker_ids:\n",
    "    for path in sorted(glob.glob(\"/raid/users/ayesilkanat/MSC/L1Arctic/cmu_us_\"+tr_id+\"_arctic/wav/*.wav\")):\n",
    "        speaker2filenames[tr_id].append(path)\n",
    "      \n",
    "        \n",
    "for tst_id in l2arctic_test_speaker_ids:\n",
    "        \n",
    "    for path in sorted(glob.glob(\"/raid/users/ayesilkanat/MSC/ArcticL2/\"+tst_id+\"/wav/*.wav\")):\n",
    "        speaker2filenames[tst_id].append(path)\n",
    "        \n",
    "        \n",
    "        \n",
    "train_path_list, in_test_path_list, out_test_path_list = [], [], []\n",
    "for speaker in train_speaker_ids:\n",
    "    path_list = speaker2filenames[speaker]\n",
    "    random.shuffle(path_list)\n",
    "    test_data_size = int(len(path_list) * test_proportion)\n",
    "    train_path_list += path_list[:-test_data_size]\n",
    "    in_test_path_list += path_list[-test_data_size:]\n",
    "for speaker in test_speaker_ids:\n",
    "    path_list = speaker2filenames[speaker]\n",
    "    out_test_path_list += path_list\n",
    "    \n",
    "    \n",
    "print(speaker2filenames.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(os.path.join(output_dir, 'in_test_files.txt'), 'w') as f:\n",
    "    for path in in_test_path_list:\n",
    "        f.write(f'{path}\\n')\n",
    "\n",
    "\n",
    "\n",
    "with open(os.path.join(output_dir, 'out_test_files.txt'), 'w') as f:\n",
    "    for path in out_test_path_list:\n",
    "        f.write(f'{path}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#with ThreadPoolExecutor(max_workers=256) as executor:\n",
    "#    future = executor.submit(spec_feature_extraction, train_path_list[0])\n",
    "#    print(future.result()[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing train set, 17067 files\n",
      "processing 0 files\n",
      "processing in_test set, 1891 files\n",
      "processing 0 files\n",
      "processing out_test set, 7918 files\n",
      "processing 0 files\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for dset, path_list in zip(['train', 'in_test', 'out_test'], \\\n",
    "        [train_path_list, in_test_path_list, out_test_path_list]):\n",
    "    print(f'processing {dset} set, {len(path_list)} files')\n",
    "    data = {}\n",
    "    output_path = os.path.join(output_dir, f'{dset}.pkl')\n",
    "    all_train_data = []\n",
    "    for i, path in enumerate(sorted(path_list)):\n",
    "        if i % 500 == 0 or i == len(path_list) - 1:\n",
    "            print(f'processing {i} files')\n",
    "         \n",
    "        filename = path.strip().split('/')[-1]\n",
    "        mel, mag = spec_feature_extraction(path)\n",
    "        data[filename] = mel\n",
    "        if dset == 'train' and i < n_utts_attr:\n",
    "            all_train_data.append(mel)\n",
    "    if dset == 'train':\n",
    "        all_train_data = np.concatenate(all_train_data)\n",
    "        mean = np.mean(all_train_data, axis=0)\n",
    "        std = np.std(all_train_data, axis=0)\n",
    "        attr = {'mean': mean, 'std': std}\n",
    "        with open(os.path.join(output_dir, 'attr.pkl'), 'wb') as f:\n",
    "            pickle.dump(attr, f)\n",
    "    for key, val in data.items():\n",
    "        val = (val - mean) / std\n",
    "        data[key] = val\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14585"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pkl_path = os.path.join(output_dir,\"train.pkl\")\n",
    "output_path = os.path.join(output_dir,\"train_\"+str(segment_size)+\".pkl\")\n",
    "\n",
    "with open(pkl_path, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "reduced_data = {key:val for key, val in data.items() if val.shape[0] > segment_size}\n",
    "\n",
    "with open(output_path, 'wb') as f:\n",
    "    pickle.dump(reduced_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del reduced_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "pickle_path = os.path.join(output_dir,\"train.pkl\")\n",
    "sample_path = os.path.join(output_dir,\"train_samples_\"+str(segment_size)+\".json\")\n",
    "n_samples = training_samples\n",
    "\n",
    "sample_single_segments(pickle_path,sample_path,segment_size,n_samples)\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "pickle_path = os.path.join(output_dir,\"in_test.pkl\")\n",
    "sample_path = os.path.join(output_dir,\"in_test_samples_\"+str(segment_size)+\".json\")\n",
    "n_samples = testing_samples\n",
    "\n",
    "sample_single_segments(pickle_path,sample_path,segment_size,n_samples)\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "pickle_path = os.path.join(output_dir,\"out_test.pkl\")\n",
    "sample_path = os.path.join(output_dir,\"out_test_samples_\"+str(segment_size)+\".json\")\n",
    "n_samples = testing_samples\n",
    "sample_single_segments(pickle_path,sample_path,segment_size,n_samples)\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
